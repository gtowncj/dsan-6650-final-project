{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"GRPO Demo – Mode A (Planner + Solver)\"\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "# Overview\n",
        "\n",
        "This notebook implements **Mode A: Planner + Solver with RL only on the Solver**.\n",
        "\n",
        "Key idea:\n",
        "- A **Planner model** (Gemma3-1B + LoRA, SFT-only / frozen) generates a *plan*.\n",
        "- A **Solver model** (Gemma3-1B + LoRA, trained with GRPO) receives *(question + plan)* and generates the final reasoning + answer.\n",
        "- **Only the Solver is trained with GRPO.**\n",
        "- All existing GSM8K reward functions (format + correctness) are preserved.\n",
        "\n",
        "This is the safest way to introduce multi-step reasoning without destabilizing RL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
        "\n",
        "import jax\n",
        "print(\"JAX backend:\", jax.default_backend())\n",
        "print(\"JAX devices:\", jax.devices())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q kagglehub ipywidgets tensorflow tensorflow_datasets tensorboardX transformers grain\n",
        "!pip install \"google-tunix[prod]==0.1.3\"\n",
        "!pip uninstall -q -y flax\n",
        "!pip install flax==0.12.0\n",
        "!pip install -q datasets wandb==0.22.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, gc, re, csv, shutil, functools\n",
        "from pprint import pprint\n",
        "from pathlib import Path\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import wandb\n",
        "import grain\n",
        "import humanize\n",
        "\n",
        "from flax import nnx\n",
        "from orbax import checkpoint as ocp\n",
        "from orbax.checkpoint import CheckpointManager, CheckpointManagerOptions\n",
        "from orbax.checkpoint.args import StandardRestore\n",
        "\n",
        "from tunix.generate import sampler as sampler_lib\n",
        "from tunix.rl import rl_cluster as rl_cluster_lib\n",
        "from tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\n",
        "from tunix.rl.rollout import base_rollout\n",
        "from tunix.sft import metrics_logger\n",
        "from tunix.models.gemma3 import params, model\n",
        "\n",
        "import qwix\n",
        "import tensorflow_datasets as tfds\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters (Unchanged from Base GRPO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_DATA_DIR = \"./data/train\"\n",
        "TEST_DATA_DIR = \"./data/test\"\n",
        "TRAIN_FRACTION = 1.0\n",
        "\n",
        "RANK = 64\n",
        "ALPHA = 64.0\n",
        "\n",
        "MESH = [(1, 4), (\"fsdp\", \"tp\")]\n",
        "\n",
        "MAX_PROMPT_LENGTH = 256\n",
        "TOTAL_GENERATION_STEPS = 512\n",
        "TEMPERATURE = 0.9\n",
        "TOP_P = 1.0\n",
        "TOP_K = 50\n",
        "NUM_GENERATIONS = 4\n",
        "\n",
        "NUM_ITERATIONS = 1\n",
        "BETA = 0.08\n",
        "EPSILON = 0.2\n",
        "\n",
        "TRAIN_MICRO_BATCH_SIZE = 4\n",
        "NUM_BATCHES = 3738\n",
        "NUM_TEST_BATCHES = 100\n",
        "NUM_EPOCHS = 1\n",
        "\n",
        "MAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\n",
        "\n",
        "LEARNING_RATE = 3e-6\n",
        "B1, B2 = 0.9, 0.99\n",
        "WEIGHT_DECAY = 0.1\n",
        "WARMUP_STEPS = 0.1 * MAX_STEPS\n",
        "MAX_GRAD_NORM = 0.1\n",
        "\n",
        "INTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt\"\n",
        "CKPT_ROOT = \"/kaggle/working/ckpts\"\n",
        "ACTOR_CKPT_DIR = os.path.join(CKPT_ROOT, \"actor\")\n",
        "\n",
        "SAVE_INTERVAL_STEPS = 500\n",
        "MAX_TO_KEEP = 4\n",
        "\n",
        "os.makedirs(INTERMEDIATE_CKPT_DIR, exist_ok=True)\n",
        "os.makedirs(ACTOR_CKPT_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Planner + Solver Prompt Templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PLAN_START = \"<plan>\"\n",
        "PLAN_END = \"</plan>\"\n",
        "\n",
        "reasoning_start = \"<reasoning>\"\n",
        "reasoning_end = \"</reasoning>\"\n",
        "solution_start = \"<answer>\"\n",
        "solution_end = \"</answer>\"\n",
        "\n",
        "PLANNER_TEMPLATE = f\"\"\"\n",
        "<start_of_turn>user\n",
        "You are a planning assistant. Produce a short numbered plan (3–5 steps) for solving the problem.\n",
        "Do NOT solve the problem.\n",
        "\n",
        "Problem:\n",
        "{{question}}\n",
        "<end_of_turn>\n",
        "<start_of_turn>planner\n",
        "{PLAN_START}\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_PROMPT = f\"\"\"\n",
        "Follow the plan. Show reasoning between {reasoning_start} and {reasoning_end}.\n",
        "Then output the final number between {solution_start} and {solution_end}.\n",
        "\"\"\"\n",
        "\n",
        "SOLVER_TEMPLATE = \"\"\"\n",
        "<start_of_turn>user\n",
        "{system_prompt}\n",
        "\n",
        "Problem:\n",
        "{question}\n",
        "\n",
        "Proposed plan:\n",
        "{plan}\n",
        "<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Loader (Unchanged GSM8K)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_hash_answer(text: str) -> str | None:\n",
        "    if \"####\" not in text:\n",
        "        return None\n",
        "    return text.split(\"####\")[1].strip()\n",
        "\n",
        "\n",
        "def get_dataset(data_dir, split=\"train\") -> grain.MapDataset:\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "    data = load_dataset(\"gsm8k\", \"main\", split=split)\n",
        "\n",
        "    def _as_text(v):\n",
        "        return v if isinstance(v, str) else v.decode(\"utf-8\")\n",
        "\n",
        "    dataset = (\n",
        "        grain.MapDataset.source(data)\n",
        "        .shuffle(seed=42)\n",
        "        .map(\n",
        "            lambda x: {\n",
        "                \"question\": _as_text(x[\"question\"]),\n",
        "                \"answer\": extract_hash_answer(_as_text(x[\"answer\"])),\n",
        "            }\n",
        "        )\n",
        "    )\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train / Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = get_dataset(TRAIN_DATA_DIR, \"train\").batch(TRAIN_MICRO_BATCH_SIZE)[:NUM_BATCHES]\n",
        "train_dataset = dataset.repeat(NUM_EPOCHS)\n",
        "test_dataset = get_dataset(TEST_DATA_DIR, \"test\").batch(TRAIN_MICRO_BATCH_SIZE)[:NUM_TEST_BATCHES]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Original Gemma Checkpoint into NNX Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_CP_PATH = params.GEMMA3_1B_IT\n",
        "config = model.ModelConfig.gemma3_1b()\n",
        "\n",
        "gemma = params.create_model_from_checkpoint(MODEL_CP_PATH, config)\n",
        "tokenizer = params.create_tokenizer()\n",
        "\n",
        "checkpointer = ocp.StandardCheckpointer()\n",
        "_, state = nnx.split(gemma)\n",
        "\n",
        "ckpt_manager = CheckpointManager(\n",
        "    INTERMEDIATE_CKPT_DIR,\n",
        "    checkpointers=checkpointer,\n",
        "    options=CheckpointManagerOptions(save_interval_steps=1, max_to_keep=1),\n",
        ")\n",
        "ckpt_manager.save(0, state)\n",
        "ckpt_manager.wait_until_finished()\n",
        "\n",
        "del gemma, state, params\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Reference Model + Apply LoRA Separately for Planner and Solver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_gemma_ref_model(ckpt_root):\n",
        "    mesh = jax.make_mesh(*MESH)\n",
        "    abs_gemma = nnx.eval_shape(lambda: model.create_model_from_checkpoint(MODEL_CP_PATH, config))\n",
        "\n",
        "    abs_state = nnx.state(abs_gemma)\n",
        "    abs_state = jax.tree.map(\n",
        "        lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n",
        "        abs_state,\n",
        "        nnx.get_named_sharding(abs_state, mesh),\n",
        "    )\n",
        "\n",
        "    ckpt_manager = CheckpointManager(\n",
        "        ckpt_root,\n",
        "        checkpointers=ocp.StandardCheckpointer(),\n",
        "        options=CheckpointManagerOptions(save_interval_steps=1, max_to_keep=1),\n",
        "    )\n",
        "\n",
        "    latest_step = ckpt_manager.latest_step()\n",
        "    restored = ckpt_manager.restore(latest_step, args=StandardRestore(abs_state))\n",
        "    graph_def, _ = nnx.split(abs_gemma)\n",
        "    gemma = nnx.merge(graph_def, restored)\n",
        "    return gemma, mesh\n",
        "\n",
        "\n",
        "def get_lora_model(base_model, mesh):\n",
        "    lora_provider = qwix.LoraProvider(\n",
        "        module_path=\".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|.*attn_vec_einsum\",\n",
        "        rank=RANK,\n",
        "        alpha=ALPHA,\n",
        "    )\n",
        "\n",
        "    lora_model = qwix.apply_lora_to_model(base_model, lora_provider, **base_model.get_model_input())\n",
        "\n",
        "    with mesh:\n",
        "        state = nnx.state(lora_model)\n",
        "        pspecs = nnx.get_partition_spec(state)\n",
        "        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
        "        nnx.update(lora_model, sharded_state)\n",
        "\n",
        "    return lora_model\n",
        "\n",
        "ref_model, mesh = get_gemma_ref_model(INTERMEDIATE_CKPT_DIR)\n",
        "\n",
        "# Separate planner and solver LoRA policies\n",
        "planner_policy = get_lora_model(ref_model, mesh)\n",
        "solver_policy = get_lora_model(ref_model, mesh)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Samplers for Planner and Solver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "planner_sampler = sampler_lib.Sampler(\n",
        "    transformer=planner_policy,\n",
        "    tokenizer=tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(cache_size=512, num_layers=ref_model.model_config.num_layers),\n",
        ")\n",
        "\n",
        "solver_sampler = sampler_lib.Sampler(\n",
        "    transformer=solver_policy,\n",
        "    tokenizer=tokenizer,\n",
        "    cache_config=sampler_lib.CacheConfig(\n",
        "        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        num_layers=ref_model.model_config.num_layers,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Planner → Solver Generation Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_plan(questions):\n",
        "    inputs = [PLANNER_TEMPLATE.format(question=q) for q in questions]\n",
        "    out = planner_sampler(\n",
        "        input_strings=inputs,\n",
        "        max_generation_steps=128,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        echo=False,\n",
        "    )\n",
        "\n",
        "    plans = []\n",
        "    for txt in out.text:\n",
        "        m = re.search(r\"<plan>(.+?)</plan>\", txt, re.DOTALL)\n",
        "        plans.append(m.group(1).strip() if m else txt.strip())\n",
        "    return plans\n",
        "\n",
        "\n",
        "def generate_with_plan(questions):\n",
        "    plans = generate_plan(questions)\n",
        "\n",
        "    solver_inputs = [\n",
        "        SOLVER_TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=q, plan=p)\n",
        "        for q, p in zip(questions, plans)\n",
        "    ]\n",
        "\n",
        "    out = solver_sampler(\n",
        "        input_strings=solver_inputs,\n",
        "        max_generation_steps=768,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_k=TOP_K,\n",
        "        top_p=TOP_P,\n",
        "        echo=False,\n",
        "    )\n",
        "\n",
        "    return plans, out.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reward Functions (Unchanged – Applied Only to Solver Output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "match_format = re.compile(rf\"^[\\\\s]{{0,}}{reasoning_start}.+?{reasoning_end}.*?{solution_start}(.+?){solution_end}[\\\\s]{{0,}}$\", re.DOTALL)\n",
        "\n",
        "def match_format_exactly(prompts, completions, **kwargs):\n",
        "    return [0 if match_format.search(r) is None else 3.0 for r in completions]\n",
        "\n",
        "\n",
        "def match_format_approximately(prompts, completions, **kwargs):\n",
        "    scores = []\n",
        "    for r in completions:\n",
        "        score = 0\n",
        "        score += 0.5 if r.count(reasoning_start) == 1 else -0.5\n",
        "        score += 0.5 if r.count(reasoning_end) == 1 else -0.5\n",
        "        score += 0.5 if r.count(solution_start) == 1 else -0.5\n",
        "        score += 0.5 if r.count(solution_end) == 1 else -0.5\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GRPO Setup (Solver Only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = optax.chain(\n",
        "    optax.clip_by_global_norm(MAX_GRAD_NORM),\n",
        "    optax.adamw(\n",
        "        learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
        "            0.0, LEARNING_RATE, WARMUP_STEPS, MAX_STEPS\n",
        "        ),\n",
        "        b1=B1,\n",
        "        b2=B2,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "    ),\n",
        ")\n",
        "\n",
        "cluster_config = rl_cluster_lib.ClusterConfig(\n",
        "    role_to_mesh={\n",
        "        rl_cluster_lib.Role.ACTOR: mesh,\n",
        "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
        "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
        "    },\n",
        "    rollout_engine=\"vanilla\",\n",
        "    offload_to_cpu=False,\n",
        "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
        "        actor_optimizer=optimizer,\n",
        "        max_steps=MAX_STEPS,\n",
        "        train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n",
        "        checkpoint_root_directory=CKPT_ROOT,\n",
        "        checkpointing_options=ocp.CheckpointManagerOptions(\n",
        "            save_interval_steps=SAVE_INTERVAL_STEPS,\n",
        "            max_to_keep=MAX_TO_KEEP,\n",
        "        ),\n",
        "        metrics_logging_options=metrics_logger.MetricsLoggerOptions(\n",
        "            log_dir=\"/tmp/content/tmp/tensorboard/grpo\",\n",
        "            flush_every_n_steps=20,\n",
        "        ),\n",
        "    ),\n",
        "    rollout_config=base_rollout.RolloutConfig(\n",
        "        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n",
        "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
        "        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n",
        "        temperature=TEMPERATURE,\n",
        "        top_p=TOP_P,\n",
        "        top_k=TOP_K,\n",
        "        eos_tokens=[1, 106],\n",
        "    ),\n",
        ")\n",
        "\n",
        "rl_cluster = rl_cluster_lib.RLCluster(\n",
        "    actor=solver_policy,\n",
        "    reference=ref_model,\n",
        "    tokenizer=tokenizer,\n",
        "    cluster_config=cluster_config,\n",
        ")\n",
        "\n",
        "grpo_trainer = GRPOLearner(\n",
        "    rl_cluster=rl_cluster,\n",
        "    reward_fns=[match_format_exactly, match_format_approximately],\n",
        "    grpo_config=GRPOConfig(\n",
        "        num_generations=NUM_GENERATIONS,\n",
        "        num_iterations=NUM_ITERATIONS,\n",
        "        beta=BETA,\n",
        "        epsilon=EPSILON,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training (Solver Only, Planner Frozen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with mesh:\n",
        "    grpo_trainer.train(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ✅ Summary\n",
        "\n",
        "This notebook implements:\n",
        "- **Planner (SFT-only, frozen)**\n",
        "- **Solver (GRPO RL)**\n",
        "- **Two-stage reasoning pipeline**\n",
        "- **Preserved GSM8K reward structure**\n",
        "- **No planner destabilization**\n",
        "\n",
        "This is the correct stepping stone before full dual-policy RL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tunix-fine-tuning-project",
      "language": "python",
      "name": "tunix-fine-tuning-project",
      "path": "/home/cjmijones/DSAN_Ubuntu/DSAN_6650/dsan-6650-final-project/.venv/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
