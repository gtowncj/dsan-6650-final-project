{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Two Step GRPO Trained Model Interactive Notebook"
      ],
      "metadata": {
        "id": "ZNRip2hjcJZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment & Setup"
      ],
      "metadata": {
        "id": "EMC5-AXm9zSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
        "\n",
        "import jax\n",
        "print(\"JAX backend:\", jax.default_backend())\n",
        "print(\"JAX devices:\", jax.devices())\n",
        "\n",
        "# --- Colab installs (leave as-is) ---\n",
        "!pip install -q kagglehub\n",
        "!pip install -q ipywidgets\n",
        "\n",
        "!pip install -q tensorflow\n",
        "!pip install -q tensorflow_datasets\n",
        "!pip install -q tensorboardX\n",
        "!pip install -q transformers\n",
        "!pip install -q grain\n",
        "!pip install \"google-tunix[prod]==0.1.3\"\n",
        "\n",
        "!pip uninstall -q -y flax\n",
        "!pip install flax==0.12.0\n",
        "\n",
        "!pip install -q datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGlE2bpz9tzZ",
        "outputId": "d714f044-7c3d-4a41-fcad-61e64641e4a8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX backend: tpu\n",
            "JAX devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]\n",
            "Requirement already satisfied: google-tunix==0.1.3 in /usr/local/lib/python3.12/dist-packages (from google-tunix[prod]==0.1.3) (0.1.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.4.1)\n",
            "Requirement already satisfied: flax>=0.11.1 in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.12.0)\n",
            "Requirement already satisfied: gcsfs in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.10.0)\n",
            "Requirement already satisfied: grain in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.2.15)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.36.0)\n",
            "Requirement already satisfied: jaxtyping in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.1.6)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.3.13)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.62.1)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.3.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.2.1)\n",
            "Requirement already satisfied: qwix in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.2.1)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.6.4)\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.9.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.57.2)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.9)\n",
            "Requirement already satisfied: jax!=0.7.2,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.0.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.1.2)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.2.6)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.11.28)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.79)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (14.2.0)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.15.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (6.0.3)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.10)\n",
            "Requirement already satisfied: jaxlib<=0.8.1,>=0.8.1 in /usr/local/lib/python3.12/dist-packages (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (0.8.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (0.5.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (1.16.3)\n",
            "Requirement already satisfied: libtpu==0.0.30.* in /usr/local/lib/python3.12/dist-packages (from jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (0.0.30)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (22.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.2.2)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.70.18)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (25.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.2.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.13.2)\n",
            "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.12/dist-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (5.2.1)\n",
            "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.12/dist-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.43.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.12/dist-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.2.2)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.12/dist-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.6.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.4.0)\n",
            "Requirement already satisfied: array-record>=0.8.1 in /usr/local/lib/python3.12/dist-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.8.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.1.2)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/dist-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.13.0)\n",
            "Requirement already satisfied: protobuf>=5.28.3 in /usr/local/lib/python3.12/dist-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (6.33.1)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from jaxtyping->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.0.3)\n",
            "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.45.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.9.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.9)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.2.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (5.9.5)\n",
            "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.17.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.2.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.10.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.7.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.22.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.8.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.23.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.2->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.2->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.2->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (2.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.19.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from google-auth-oauthlib->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.0.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=2.27.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.28.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.5.0)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.8.0)\n",
            "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.7.1)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.90)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.6.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (25.1.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.14.0)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.20.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from promise->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.17.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.12/dist-packages (from simple_parsing->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.12/dist-packages (from tensorflow-metadata->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.72.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (75.2.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.1.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.26.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.3.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.3.1)\n",
            "Collecting flax==0.12.0\n",
            "  Using cached flax-0.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from flax==0.12.0) (2.0.2)\n",
            "Requirement already satisfied: jax>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from flax==0.12.0) (0.8.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from flax==0.12.0) (1.1.2)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (from flax==0.12.0) (0.2.6)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.12/dist-packages (from flax==0.12.0) (0.11.28)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from flax==0.12.0) (0.1.79)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax==0.12.0) (14.2.0)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from flax==0.12.0) (4.15.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from flax==0.12.0) (6.0.3)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax==0.12.0) (0.1.10)\n",
            "Requirement already satisfied: jaxlib<=0.8.1,>=0.8.1 in /usr/local/lib/python3.12/dist-packages (from jax>=0.7.1->flax==0.12.0) (0.8.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.7.1->flax==0.12.0) (0.5.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.7.1->flax==0.12.0) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax>=0.7.1->flax==0.12.0) (1.16.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax==0.12.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax==0.12.0) (2.19.2)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from optax->flax==0.12.0) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax->flax==0.12.0) (0.1.90)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax==0.12.0) (1.13.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax==0.12.0) (1.6.0)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax==0.12.0) (25.1.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax==0.12.0) (6.33.1)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax==0.12.0) (4.14.0)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax==0.12.0) (3.20.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint->flax==0.12.0) (5.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->flax==0.12.0) (75.2.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->flax==0.12.0) (1.1.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax==0.12.0) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax==0.12.0) (2025.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax==0.12.0) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax==0.12.0) (3.23.0)\n",
            "Using cached flax-0.12.0-py3-none-any.whl (466 kB)\n",
            "Installing collected packages: flax\n",
            "Successfully installed flax-0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Constants"
      ],
      "metadata": {
        "id": "3NLtb0O893RL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "from flax import nnx\n",
        "from orbax import checkpoint as ocp\n",
        "from orbax.checkpoint import CheckpointManager, CheckpointManagerOptions\n",
        "from orbax.checkpoint.args import StandardRestore\n",
        "\n",
        "from tunix.generate import sampler as sampler_lib\n",
        "from tunix.models.gemma3 import params, model\n",
        "\n",
        "import qwix\n",
        "import tensorflow_datasets as tfds\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "4KFm0SX09xTq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------\n",
        "#   Global constants\n",
        "# --------------------\n",
        "\n",
        "TRAIN_DATA_DIR = \"./data/train\"\n",
        "TEST_DATA_DIR  = \"./data/test\"\n",
        "TRAIN_FRACTION = 1.0\n",
        "\n",
        "RANK  = 64\n",
        "ALPHA = 64.0\n",
        "\n",
        "MESH = [(1, 1), (\"fsdp\", \"tp\")]\n",
        "\n",
        "MAX_PROMPT_LENGTH = 256\n",
        "\n",
        "TEMPERATURE = 0.9\n",
        "TOP_P       = 1.0\n",
        "TOP_K       = 50\n",
        "\n",
        "TRAIN_MICRO_BATCH_SIZE = 2\n",
        "NUM_BATCHES      = 1000\n",
        "NUM_TEST_BATCHES = 100\n",
        "NUM_EPOCHS       = 1\n",
        "\n",
        "# These are only used for deriving max steps in training; here, mainly kept\n",
        "# to keep directory structure consistent with RL runs.\n",
        "NUM_ITERATIONS = 1\n",
        "MAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\n",
        "\n",
        "# --------------------\n",
        "#   Checkpoint paths\n",
        "# --------------------\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "CKPT_ROOT = \"/content/drive/MyDrive/tunix_ckpts_modeB\"\n",
        "\n",
        "PLANNER_CKPT_ROOT = f\"{CKPT_ROOT}/planner\"\n",
        "SOLVER_CKPT_ROOT  = f\"{CKPT_ROOT}/solver\"\n",
        "\n",
        "INTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt\"\n",
        "\n",
        "PLANNER_ACTOR_CKPT_DIR = os.path.join(PLANNER_CKPT_ROOT, \"actor\")\n",
        "SOLVER_ACTOR_CKPT_DIR  = os.path.join(SOLVER_CKPT_ROOT, \"actor\")\n",
        "\n",
        "for d in [INTERMEDIATE_CKPT_DIR, PLANNER_CKPT_ROOT, SOLVER_CKPT_ROOT,\n",
        "          PLANNER_ACTOR_CKPT_DIR, SOLVER_ACTOR_CKPT_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "SAVE_INTERVAL_STEPS = 500\n",
        "MAX_TO_KEEP = 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1ilndO39-9f",
        "outputId": "8ac78d6e-e65d-48e5-e544-162022863b0f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------\n",
        "#   Prompt constants\n",
        "# --------------------\n",
        "\n",
        "PLAN_START = \"<plan>\"\n",
        "PLAN_END   = \"</plan>\"\n",
        "\n",
        "reasoning_start = \"<reasoning>\"\n",
        "reasoning_end   = \"</reasoning>\"\n",
        "solution_start  = \"<answer>\"\n",
        "solution_end    = \"</answer>\"\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"Follow the plan. Show each step of following the plan between \"\n",
        "    \"<reasoning> and </reasoning>. Then output the final number between \"\n",
        "    \"<answer> and </answer>.\"\n",
        ")\n",
        "\n",
        "PLANNER_TEMPLATE = f\"\"\"\n",
        "<start_of_turn>user\n",
        "You are a planning assistant. Produce a short numbered plan (3–5 steps)\n",
        "for solving the problem. Do NOT solve the problem.\n",
        "\n",
        "Problem:\n",
        "{{question}}\n",
        "<end_of_turn>\n",
        "\n",
        "<start_of_turn>planner\n",
        "{PLAN_START}\n",
        "\"\"\"\n",
        "\n",
        "# IMPORTANT: This matches the *actual* training usage:\n",
        "# SOLVER_TEMPLATE.format(question=q, plan=p)\n",
        "SOLVER_TEMPLATE = f\"\"\"\n",
        "You are a mathematical reasoning agent.\n",
        "\n",
        "Your task:\n",
        "1. Follow the provided plan EXACTLY.\n",
        "2. Write detailed reasoning in the <reasoning>...</reasoning> block.\n",
        "3. Place ONLY the final numeric answer in <answer>...</answer>.\n",
        "4. After </solution>, STOP immediately.\n",
        "\n",
        "Response format:\n",
        "<solution>\n",
        "<reasoning>\n",
        "[step-by-step reasoning following the plan; do NOT skip steps]\n",
        "</reasoning>\n",
        "<answer>\n",
        "[FINAL NUMERIC ANSWER ONLY]\n",
        "</answer>\n",
        "</solution>\n",
        "\n",
        "Problem:\n",
        "{{question}}\n",
        "\n",
        "Plan:\n",
        "{{plan}}\n",
        "\n",
        "Begin.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vNgoAMIS-DTE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Gemma Setup"
      ],
      "metadata": {
        "id": "rmZ_WMJs-C1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_CP_PATH = params.GEMMA3_1B_IT\n",
        "config        = model.ModelConfig.gemma3_1b()\n",
        "tokenizer     = params.create_tokenizer()\n",
        "\n",
        "# Save a one-time intermediate checkpoint for Gemma base weights\n",
        "gemma = params.create_model_from_checkpoint(MODEL_CP_PATH, config)\n",
        "checkpointer = ocp.StandardCheckpointer()\n",
        "_, state = nnx.split(gemma)\n",
        "\n",
        "ckpt_manager = CheckpointManager(\n",
        "    INTERMEDIATE_CKPT_DIR,\n",
        "    checkpointers=checkpointer,\n",
        "    options=CheckpointManagerOptions(save_interval_steps=1, max_to_keep=1),\n",
        ")\n",
        "ckpt_manager.save(0, state)\n",
        "ckpt_manager.wait_until_finished()\n",
        "\n",
        "del gemma, state, params\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdUBvQ2S-eng",
        "outputId": "25b88d53-109d-40ae-d54a-6052e1f2c8f2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`StandardCheckpointHandler` expects a target tree to be provided for restore. Not doing so is generally UNSAFE unless you know the present topology to be the same one as the checkpoint was saved under.\n",
            "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/guides/checkpoint/api_refactor.html to migrate.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Helpers"
      ],
      "metadata": {
        "id": "m0hS8If8-k4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tunix.models.gemma3 import params, model\n",
        "\n",
        "def enforce_stop_strings(text, stops=(\"</answer>\",)):\n",
        "    \"\"\"Cut off generation at the first stop string (inclusive).\"\"\"\n",
        "    for s in stops:\n",
        "        if s in text:\n",
        "            return text.split(s)[0] + s\n",
        "    return text\n",
        "\n",
        "def get_gemma_ref_model(ckpt_root):\n",
        "    \"\"\"\n",
        "    Loads the *base Gemma model weights* (no LoRA).\n",
        "    Returns: (restored_model, mesh)\n",
        "    \"\"\"\n",
        "    mesh = jax.make_mesh(*MESH)\n",
        "\n",
        "    # 1. Build abstract model\n",
        "    abs_gemma = nnx.eval_shape(\n",
        "        lambda: params.create_model_from_checkpoint(MODEL_CP_PATH, config)\n",
        "    )\n",
        "\n",
        "    # 2. Abstract state with sharding info\n",
        "    abs_state = nnx.state(abs_gemma)\n",
        "    abs_state = jax.tree.map(\n",
        "        lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n",
        "        abs_state,\n",
        "        nnx.get_named_sharding(abs_state, mesh),\n",
        "    )\n",
        "\n",
        "    # 3. Restore base checkpoint\n",
        "    ckpt_manager = CheckpointManager(\n",
        "        ckpt_root,\n",
        "        checkpointers=ocp.StandardCheckpointer(),\n",
        "        options=CheckpointManagerOptions(save_interval_steps=1, max_to_keep=1),\n",
        "    )\n",
        "\n",
        "    latest_step = ckpt_manager.latest_step()\n",
        "    if latest_step is None:\n",
        "        raise FileNotFoundError(f\"No checkpoints found in {ckpt_root}\")\n",
        "\n",
        "    restored_state = ckpt_manager.restore(\n",
        "        latest_step,\n",
        "        args=StandardRestore(abs_state),\n",
        "    )\n",
        "\n",
        "    # 4. Merge abstract graph + restored state\n",
        "    graph_def, _ = nnx.split(abs_gemma)\n",
        "    gemma = nnx.merge(graph_def, restored_state)\n",
        "\n",
        "    print(f\"✅ Loaded base Gemma model from step {latest_step}\")\n",
        "    return gemma, mesh\n",
        "\n",
        "\n",
        "def get_lora_model(base_model, mesh):\n",
        "    \"\"\"\n",
        "    Create a fresh LoRA-adapted policy from the base Gemma weights.\n",
        "    \"\"\"\n",
        "    lora_provider = qwix.LoraProvider(\n",
        "        module_path=(\n",
        "            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|\"\n",
        "            \".*up_proj|.*attn_vec_einsum\"\n",
        "        ),\n",
        "        rank=RANK,\n",
        "        alpha=ALPHA,\n",
        "    )\n",
        "\n",
        "    lora_model = qwix.apply_lora_to_model(\n",
        "        base_model,\n",
        "        lora_provider,\n",
        "        **base_model.get_model_input(),\n",
        "    )\n",
        "\n",
        "    # Re-shard after LoRA insertion\n",
        "    with mesh:\n",
        "        state   = nnx.state(lora_model)\n",
        "        pspecs  = nnx.get_partition_spec(state)\n",
        "        sharded = jax.lax.with_sharding_constraint(state, pspecs)\n",
        "        nnx.update(lora_model, sharded)\n",
        "\n",
        "    return lora_model\n",
        "\n",
        "\n",
        "def load_lora_exact(policy, ckpt_dir, label=\"\"):\n",
        "    \"\"\"\n",
        "    Load LoRA-only weights from a Tunix RL actor checkpoint directory into `policy`.\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== Loading {label} LoRA ===\")\n",
        "\n",
        "    steps = sorted([int(x) for x in os.listdir(ckpt_dir) if x.isdigit()])\n",
        "    if not steps:\n",
        "        raise ValueError(f\"No valid checkpoint folders in {ckpt_dir}\")\n",
        "    latest = steps[-1]\n",
        "\n",
        "    ckpt_path = os.path.join(ckpt_dir, str(latest), \"model_params\")\n",
        "    print(f\"→ Using checkpoint: {ckpt_path}\")\n",
        "\n",
        "    # Extract the LoRA tree\n",
        "    lora_tree = nnx.state(policy, nnx.LoRAParam)\n",
        "\n",
        "    # Abstract target\n",
        "    abs_target = jax.tree.map(\n",
        "        lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
        "        lora_tree,\n",
        "    )\n",
        "\n",
        "    checkpointer = ocp.StandardCheckpointer()\n",
        "    restored = checkpointer.restore(ckpt_path, target=abs_target)\n",
        "\n",
        "    # Inject into policy\n",
        "    nnx.update(\n",
        "        policy,\n",
        "        jax.tree.map(lambda old, new: new, lora_tree, restored),\n",
        "    )\n",
        "\n",
        "    print(f\"✓ {label} LoRA loaded successfully.\")\n",
        "\n",
        "\n",
        "# Optional debug helpers\n",
        "def lora_l2(model):\n",
        "    lora_state = nnx.state(model, nnx.LoRAParam)\n",
        "    leaves = jax.tree.leaves(lora_state)\n",
        "    return sum(float(jnp.linalg.norm(x)) for x in leaves)\n",
        "\n",
        "def lora_diff(model_a, model_b):\n",
        "    a = nnx.state(model_a, nnx.LoRAParam)\n",
        "    b = nnx.state(model_b, nnx.LoRAParam)\n",
        "    diffs = jax.tree.map(lambda x, y: float(jnp.linalg.norm(x - y)), a, b)\n",
        "    leaves = jax.tree.leaves(diffs)\n",
        "    return sum(leaves)\n",
        "\n",
        "# =========================\n",
        "#   4. Load Policies & Samplers\n",
        "# =========================\n",
        "\n",
        "def load_policies_and_samplers():\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      baseline_sampler: baseline LoRA-injected model (no RL LoRA)\n",
        "      planner_sampler:  planner with TRAINED LoRA\n",
        "      solver_sampler:   solver with TRAINED LoRA\n",
        "    \"\"\"\n",
        "\n",
        "    # 1) Load base Gemma\n",
        "    ref_model, mesh = get_gemma_ref_model(INTERMEDIATE_CKPT_DIR)\n",
        "\n",
        "    # 2) Inject LoRA heads\n",
        "    baseline_policy = get_lora_model(ref_model, mesh)\n",
        "    planner_policy  = get_lora_model(ref_model, mesh)\n",
        "    solver_policy   = get_lora_model(ref_model, mesh)\n",
        "\n",
        "    # 3) Load TRAINED LoRA weights into planner/solver\n",
        "    load_lora_exact(planner_policy, PLANNER_ACTOR_CKPT_DIR, \"planner\")\n",
        "    load_lora_exact(solver_policy,  SOLVER_ACTOR_CKPT_DIR,  \"solver\")\n",
        "\n",
        "    print(\"baseline L2:\", lora_l2(baseline_policy))\n",
        "    print(\"planner  L2:\", lora_l2(planner_policy))\n",
        "    print(\"solver   L2:\", lora_l2(solver_policy))\n",
        "    print(\"Δ(planner-baseline):\", lora_diff(planner_policy, baseline_policy))\n",
        "    print(\"Δ(solver-baseline): \", lora_diff(solver_policy, baseline_policy))\n",
        "    print(\"Δ(solver-planner):  \", lora_diff(solver_policy, planner_policy))\n",
        "\n",
        "    # 4) Build samplers AFTER LoRA is loaded.\n",
        "    #\n",
        "    # Planner: short generations, small-ish cache.\n",
        "    baseline_sampler = sampler_lib.Sampler(\n",
        "        transformer=baseline_policy,\n",
        "        tokenizer=tokenizer,\n",
        "        cache_config=sampler_lib.CacheConfig(\n",
        "            cache_size=MAX_PROMPT_LENGTH + 256,  # prompt + ~128-200 tokens\n",
        "            num_layers=config.num_layers,\n",
        "            num_kv_heads=config.num_kv_heads,\n",
        "            head_dim=config.head_dim,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    planner_sampler = sampler_lib.Sampler(\n",
        "        transformer=planner_policy,\n",
        "        tokenizer=tokenizer,\n",
        "        cache_config=sampler_lib.CacheConfig(\n",
        "            cache_size=MAX_PROMPT_LENGTH + 256,  # same as baseline\n",
        "            num_layers=config.num_layers,\n",
        "            num_kv_heads=config.num_kv_heads,\n",
        "            head_dim=config.head_dim,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Solver: longer generations; make sure cache_size >= MAX_PROMPT_LENGTH + max_gen\n",
        "    solver_max_gen = 768\n",
        "    solver_sampler = sampler_lib.Sampler(\n",
        "        transformer=solver_policy,\n",
        "        tokenizer=tokenizer,\n",
        "        cache_config=sampler_lib.CacheConfig(\n",
        "            cache_size=MAX_PROMPT_LENGTH + solver_max_gen + 256,\n",
        "            num_layers=config.num_layers,\n",
        "            num_kv_heads=config.num_kv_heads,\n",
        "            head_dim=config.head_dim,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    return baseline_sampler, planner_sampler, solver_sampler\n",
        "\n",
        "baseline_sampler, planner_sampler, solver_sampler = load_policies_and_samplers()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FToK15VS-lPe",
        "outputId": "b3a113bb-9940-4755-a62c-f4b2b8fad4bf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3500741403.py:15: DeprecationWarning: The default axis_types will change in JAX v0.9.0 to jax.sharding.AxisType.Explicit. To maintain the old behavior, pass `axis_types=(jax.sharding.AxisType.Auto,) * len(axis_names)`. To opt-into the new behavior, pass `axis_types=(jax.sharding.AxisType.Explicit,) * len(axis_names)\n",
            "  mesh = jax.make_mesh(*MESH)\n",
            "WARNING:absl:`StandardCheckpointHandler` expects a target tree to be provided for restore. Not doing so is generally UNSAFE unless you know the present topology to be the same one as the checkpoint was saved under.\n",
            "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/guides/checkpoint/api_refactor.html to migrate.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded base Gemma model from step 0\n",
            "\n",
            "=== Loading planner LoRA ===\n",
            "→ Using checkpoint: /content/drive/MyDrive/tunix_ckpts_modeB/planner/actor/1000/model_params\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/orbax/checkpoint/_src/serialization/jax_array_handlers.py:701: UserWarning: Sharding info not provided when restoring. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ planner LoRA loaded successfully.\n",
            "\n",
            "=== Loading solver LoRA ===\n",
            "→ Using checkpoint: /content/drive/MyDrive/tunix_ckpts_modeB/solver/actor/1000/model_params\n",
            "✓ solver LoRA loaded successfully.\n",
            "baseline L2: 1766.375\n",
            "planner  L2: 1768.9099426269531\n",
            "solver   L2: 1769.4983825683594\n",
            "Δ(planner-baseline): 3.2076034545898438\n",
            "Δ(solver-baseline):  3.5033798217773438\n",
            "Δ(solver-planner):   4.628963470458984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Loader (GSM8K)"
      ],
      "metadata": {
        "id": "JFW4yGbC-yXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_hash_answer(text: str) -> str | None:\n",
        "    if \"####\" not in text:\n",
        "        return None\n",
        "    return text.split(\"####\")[1].strip()\n",
        "\n",
        "def get_dataset(data_dir, split=\"train\"):\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "    data = load_dataset(\"gsm8k\", \"main\", split=split)\n",
        "\n",
        "    def _as_text(v):\n",
        "        return v if isinstance(v, str) else v.decode(\"utf-8\")\n",
        "\n",
        "    import grain\n",
        "    dataset = (\n",
        "        grain.MapDataset.source(data)\n",
        "        .shuffle(seed=42)\n",
        "        .map(\n",
        "            lambda x: {\n",
        "                \"question\": _as_text(x[\"question\"]),\n",
        "                \"answer\": extract_hash_answer(_as_text(x[\"answer\"])),\n",
        "            }\n",
        "        )\n",
        "    )\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "I8ViMUHW-yvy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = (\n",
        "    get_dataset(TRAIN_DATA_DIR, \"train\")\n",
        "    .batch(TRAIN_MICRO_BATCH_SIZE)[:NUM_BATCHES]\n",
        "    .repeat(NUM_EPOCHS)\n",
        ")\n",
        "\n",
        "test_dataset = (\n",
        "    get_dataset(TEST_DATA_DIR, \"test\")\n",
        "    .batch(TRAIN_MICRO_BATCH_SIZE)[:NUM_TEST_BATCHES]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywEvaEA--45Z",
        "outputId": "b0a2c251-a89e-4e31-aa75-f41a011c833f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Planner & Solver Wrappers"
      ],
      "metadata": {
        "id": "ic6kOizL-7Va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_plan_with_sampler(questions, planner_sampler):\n",
        "    \"\"\"\n",
        "    Planner stage:\n",
        "    Uses PLANNER_TEMPLATE and the given planner_sampler to generate <plan> text.\n",
        "    \"\"\"\n",
        "    inputs = [PLANNER_TEMPLATE.format(question=q) for q in questions]\n",
        "\n",
        "    out = planner_sampler(\n",
        "        input_strings=inputs,\n",
        "        max_generation_steps=128,     # plans are short\n",
        "        temperature=0.3,              # more deterministic / obedient\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        echo=False,\n",
        "    )\n",
        "\n",
        "    plans = []\n",
        "    for txt in out.text:\n",
        "        # Trim at PLAN_END if present\n",
        "        if PLAN_END in txt:\n",
        "            txt = txt.split(PLAN_END)[0] + PLAN_END\n",
        "\n",
        "        m = re.search(r\"<plan>(.*?)</plan>\", txt, re.DOTALL)\n",
        "        clean = m.group(1).strip() if m else txt.strip()\n",
        "\n",
        "        # Strip tags & trailing junk\n",
        "        clean = re.sub(r\"</?plan>\", \"\", clean)\n",
        "        clean = clean.split(\"<end_of_turn>\")[0]\n",
        "        plans.append(clean)\n",
        "\n",
        "    return plans\n",
        "\n",
        "\n",
        "def generate_solution_with_sampler(\n",
        "    questions,\n",
        "    plans,\n",
        "    solver_sampler,\n",
        "    temperature=0.2,\n",
        "    top_k=TOP_K,\n",
        "    top_p=TOP_P,\n",
        "    max_generation_steps=768,\n",
        "):\n",
        "    \"\"\"\n",
        "    Solver stage:\n",
        "    Builds SOLVER_TEMPLATE(question, plan) and generates full CoT + answer.\n",
        "    \"\"\"\n",
        "    solver_inputs = [\n",
        "        SOLVER_TEMPLATE.format(question=q, plan=p)\n",
        "        for q, p in zip(questions, plans)\n",
        "    ]\n",
        "\n",
        "    out = solver_sampler(\n",
        "        input_strings=solver_inputs,\n",
        "        max_generation_steps=max_generation_steps,\n",
        "        max_prompt_length=MAX_PROMPT_LENGTH,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        echo=False,\n",
        "    )\n",
        "\n",
        "    outputs = [enforce_stop_strings(t) for t in out.text]\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "UnKd1FC8-7to"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Helpers"
      ],
      "metadata": {
        "id": "fkG0Z-IQ_KPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regex for extracting numeric answer from within <answer>...</answer>\n",
        "match_numbers = re.compile(\n",
        "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\",\n",
        "    flags=re.MULTILINE | re.DOTALL,\n",
        ")\n",
        "\n",
        "# Regex for format correctness: <reasoning>...</reasoning> then <answer>...</answer>\n",
        "match_format = re.compile(\n",
        "    r\"<reasoning>[\\s\\S]*?</reasoning>\\s*<answer>[\\s\\S]*?</answer>\",\n",
        "    re.MULTILINE,\n",
        ")\n",
        "\n",
        "def evaluate_pipeline(\n",
        "    dataset,\n",
        "    planner_sampler,\n",
        "    solver_sampler,\n",
        "    temperature=0.2,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    num_passes=1,\n",
        "    record_incorrect=False,\n",
        "    record_correct=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Full 2-stage pipeline evaluation:\n",
        "      planner_sampler → generates plan\n",
        "      solver_sampler  → generates reasoning + answer from that plan\n",
        "\n",
        "    Returns a metrics dict and an optional list of collected examples.\n",
        "    \"\"\"\n",
        "\n",
        "    total           = 0\n",
        "    exact_correct   = 0\n",
        "    partial_correct = 0\n",
        "    format_correct  = 0\n",
        "\n",
        "    collected = []\n",
        "\n",
        "    for batch in tqdm(dataset):\n",
        "        questions = batch[\"question\"]\n",
        "        answers   = batch[\"answer\"]\n",
        "\n",
        "        multi_outputs = [[] for _ in range(len(questions))]\n",
        "\n",
        "        for _ in range(num_passes):\n",
        "            # 1) Planner stage\n",
        "            plans = generate_plan_with_sampler(questions, planner_sampler)\n",
        "\n",
        "            # 2) Solver stage\n",
        "            outputs = generate_solution_with_sampler(\n",
        "                questions,\n",
        "                plans,\n",
        "                solver_sampler,\n",
        "                temperature=temperature,\n",
        "                top_k=top_k,\n",
        "                top_p=top_p,\n",
        "            )\n",
        "\n",
        "            for i, out in enumerate(outputs):\n",
        "                multi_outputs[i].append(out)\n",
        "\n",
        "        # 3) Evaluate each question\n",
        "        for q, gold, responses in zip(questions, answers, multi_outputs):\n",
        "            total += 1\n",
        "            gold = gold.strip()\n",
        "\n",
        "            is_exact  = False\n",
        "            is_partial = False\n",
        "            is_format  = False\n",
        "\n",
        "            for resp in responses:\n",
        "                # Extract numeric answer\n",
        "                m = match_numbers.search(resp)\n",
        "                pred_str = m.group(1).strip() if m else None\n",
        "\n",
        "                try:\n",
        "                    pred_val = float(pred_str) if pred_str is not None else None\n",
        "                    gold_val = float(gold)\n",
        "                except Exception:\n",
        "                    pred_val = None\n",
        "\n",
        "                # Exact correctness\n",
        "                if pred_val is not None and pred_val == gold_val:\n",
        "                    is_exact = True\n",
        "\n",
        "                # Partial correctness (±10%)\n",
        "                if pred_val is not None:\n",
        "                    ratio = pred_val / gold_val\n",
        "                    if 0.9 <= ratio <= 1.1:\n",
        "                        is_partial = True\n",
        "\n",
        "                # Format correctness: <reasoning>...</reasoning><answer>...</answer>\n",
        "                if match_format.search(resp):\n",
        "                    is_format = True\n",
        "\n",
        "                if is_exact and is_partial and is_format:\n",
        "                    break\n",
        "\n",
        "            exact_correct   += int(is_exact)\n",
        "            partial_correct += int(is_partial)\n",
        "            format_correct  += int(is_format)\n",
        "\n",
        "            if record_correct and is_exact:\n",
        "                collected.append((q, gold, responses))\n",
        "            if record_incorrect and not is_exact:\n",
        "                collected.append((q, gold, responses))\n",
        "\n",
        "    metrics = {\n",
        "        \"total\": total,\n",
        "        \"exact_correct\":   exact_correct,\n",
        "        \"partial_correct\": partial_correct,\n",
        "        \"format_correct\":  format_correct,\n",
        "        \"exact_accuracy\":   100.0 * exact_correct   / total,\n",
        "        \"partial_accuracy\": 100.0 * partial_correct / total,\n",
        "        \"format_accuracy\":  100.0 * format_correct  / total,\n",
        "    }\n",
        "\n",
        "    return metrics, collected"
      ],
      "metadata": {
        "id": "VYMlRs4N_Kp8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trained Evaluations"
      ],
      "metadata": {
        "id": "M3hP9z5a_PXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline pipeline: planner + solver both baseline\n",
        "# baseline_metrics, baseline_collected = evaluate_pipeline(\n",
        "#     test_dataset,\n",
        "#     planner_sampler=baseline_sampler,\n",
        "#     solver_sampler=baseline_sampler,\n",
        "#     num_passes=1,\n",
        "# )\n",
        "\n",
        "# print(\"BASELINE METRICS:\")\n",
        "# for k, v in baseline_metrics.items():\n",
        "#     print(f\"  {k}: {v}\")\n",
        "\n",
        "# Trained pipeline: planner + solver both trained\n",
        "trained_metrics, trained_collected = evaluate_pipeline(\n",
        "    test_dataset,\n",
        "    planner_sampler=planner_sampler,\n",
        "    solver_sampler=solver_sampler,\n",
        "    num_passes=1,\n",
        ")\n",
        "\n",
        "print(\"\\nTRAINED METRICS:\")\n",
        "for k, v in trained_metrics.items():\n",
        "    print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mxshh4MJ_QfV",
        "outputId": "0d4cca6b-30b8-4e89-d784-ce91678a2e14"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [06:06<00:00,  3.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TRAINED METRICS:\n",
            "  total: 200\n",
            "  exact_correct: 77\n",
            "  partial_correct: 84\n",
            "  format_correct: 95\n",
            "  exact_accuracy: 38.5\n",
            "  partial_accuracy: 42.0\n",
            "  format_accuracy: 47.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optional: Trace One Example"
      ],
      "metadata": {
        "id": "EPej9dQ0_S56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trace_pipeline(question, true_answer, planner_sampler, solver_sampler):\n",
        "    print(\"\\n--- Tracing Pipeline ---\")\n",
        "    print(f\"Question: {question}\")\n",
        "\n",
        "    planner_input = PLANNER_TEMPLATE.format(question=question)\n",
        "    print(f\"\\nPlanner Input:\\n{planner_input}\")\n",
        "\n",
        "    plans = generate_plan_with_sampler([question], planner_sampler)\n",
        "    plan  = plans[0]\n",
        "\n",
        "    print(f\"\\nGenerated Plan:\\n{PLAN_START}\\n{plan}\\n{PLAN_END}\")\n",
        "\n",
        "    outputs = generate_solution_with_sampler([question], [plan], solver_sampler)\n",
        "    solver_output = outputs[0]\n",
        "\n",
        "    print(f\"\\nSolver Output:\\n{solver_output}\")\n",
        "\n",
        "    # crude check-answer view\n",
        "    m = match_numbers.search(solver_output)\n",
        "    pred_str = m.group(1).strip() if m else None\n",
        "    print(f\"\\nExtracted Predicted Answer: {pred_str}\")\n",
        "    print(f\"True Answer: {true_answer}\")\n",
        "    print(\"--- End Tracing Pipeline ---\")\n",
        "\n",
        "# Example: trace the first sample from test set with trained pipeline\n",
        "first_sample   = next(iter(test_dataset.shuffle(seed=65)))\n",
        "first_question = first_sample[\"question\"][0]\n",
        "first_answer   = first_sample[\"answer\"][0]\n",
        "\n",
        "trace_pipeline(first_question, first_answer, planner_sampler, solver_sampler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jozqdRzW_Uay",
        "outputId": "4ea2dd49-0873-4348-d69a-91d7762e492c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Tracing Pipeline ---\n",
            "Question: Bill is ordering a new truck. He has decided to purchase a two-ton truck with several added features: a king cab upgrade, a towing package, leather seats, running boards, and the upgraded exterior light package.  The base price of the truck is $30,000, and the other features are at extra cost. The king cab is an extra $7,500, leather seats are one-third the cost of the king cab upgrade, running boards are $500 less than the leather seats, and the upgraded exterior light package is $1500.  What is the total cost of Bill's new truck, in dollars?\n",
            "\n",
            "Planner Input:\n",
            "\n",
            "<start_of_turn>user\n",
            "You are a planning assistant. Produce a short numbered plan (3–5 steps)\n",
            "for solving the problem. Do NOT solve the problem.\n",
            "\n",
            "Problem:\n",
            "Bill is ordering a new truck. He has decided to purchase a two-ton truck with several added features: a king cab upgrade, a towing package, leather seats, running boards, and the upgraded exterior light package.  The base price of the truck is $30,000, and the other features are at extra cost. The king cab is an extra $7,500, leather seats are one-third the cost of the king cab upgrade, running boards are $500 less than the leather seats, and the upgraded exterior light package is $1500.  What is the total cost of Bill's new truck, in dollars?\n",
            "<end_of_turn>\n",
            "\n",
            "<start_of_turn>planner\n",
            "<plan>\n",
            "\n",
            "\n",
            "Generated Plan:\n",
            "<plan>\n",
            "1.  Calculate the cost of the king cab upgrade.\n",
            "2.  Calculate the cost of the leather seats.\n",
            "3.  Calculate the cost of the running boards.\n",
            "4.  Calculate the cost of the upgraded exterior light package.\n",
            "5.  Sum the costs of all the features to determine the total cost of the truck.\n",
            "\n",
            "</plan>\n",
            "\n",
            "Solver Output:\n",
            "<solution>\n",
            "The cost of the king cab upgrade is $7,500.\n",
            "The cost of the leather seats is (1/3) * $7,500 = $2,500.\n",
            "The cost of the running boards is $500 less than the leather seats, so $2,500 - $500 = $2,000.\n",
            "The cost of the upgraded exterior light package is $1500.\n",
            "The total cost of all the features is $7,500 + $2,500 + $2,000 + $1500 = $13,000.\n",
            "<reasoning>\n",
            "The plan is to calculate the cost of each feature individually and then sum them up.\n",
            "First, we calculate the cost of the king cab upgrade, which is $7,500.\n",
            "Next, we calculate the cost of the leather seats, which is (1/3) * $7,500 = $2,500.\n",
            "Then, we calculate the cost of the running boards, which is $500 less than the leather seats, so $2,500 - $500 = $2,000.\n",
            "Finally, we calculate the cost of the upgraded exterior light package, which is $1500.\n",
            "The total cost of the truck is the sum of these costs: $7,500 + $2,500 + $2,000 + $1500 = $13,000.\n",
            "</reasoning>\n",
            "<answer>\n",
            "13000\n",
            "</answer>\n",
            "\n",
            "Extracted Predicted Answer: 13000\n",
            "True Answer: 43,500\n",
            "--- End Tracing Pipeline ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r1-QsUnfKN2p"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}